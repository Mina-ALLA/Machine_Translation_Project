{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine translation project.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOgBj6qn511e1LuwI7GEWG6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mina-ALLA/Machine_Translation_Project/blob/master/Machine_translation_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCRQuq9r5-KW",
        "colab_type": "text"
      },
      "source": [
        "**Building machine translation (from English to French) using seq2seq model**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxntzjrM9Ysd",
        "colab_type": "text"
      },
      "source": [
        "**Introduction**\n",
        "\n",
        "In this notebook, we will build a deep neural network that functions as part of an end-to-end machine translation pipeline. this pipeline will accept English text as input and return the French translation.\n",
        "\n",
        "**Preprocess** : we will convert text to sequence of integers.\n",
        "\n",
        "**Models** : Create models which accepts a sequence of integers as input and returns a probability distribution over possible translations. After learning about the basic types of neural networks that are often used for machine translation, we will engage in our own investigations, to design the final model!\n",
        "\n",
        "**Prediction** : Runing the model on English text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYu1nml_8K53",
        "colab_type": "code",
        "outputId": "d337ab35-7092-4315-80fb-03064236b647",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import collections\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout, LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import sparse_categorical_crossentropy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_vtmtXD-HvG",
        "colab_type": "text"
      },
      "source": [
        "We should always verify access to the GPU to speed up the learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3POlnge-lAD",
        "colab_type": "text"
      },
      "source": [
        "**DataSet**\n",
        "\n",
        "**Load Data**\n",
        "\n",
        "The data is located in small_vocab_en and small_vocab_fr. The small_vocab_en file contains English sentences with their French translations in the small_vocab_fr file. we are going to load the English and French data from these files in my github from running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UEZldXhOf4A",
        "colab_type": "code",
        "outputId": "b6be7909-330b-434f-fa2c-f38a4de942e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "import urllib3\n",
        "\n",
        "link1 = 'https://raw.githubusercontent.com/Mina-ALLA/Machine_Translation_Project/master/small_vocab_en.txt'\n",
        "link2 = 'https://raw.githubusercontent.com/Mina-ALLA/Machine_Translation_Project/master/small_vocab_fr.txt'\n",
        "http = urllib3.PoolManager()\n",
        "response_eng = http.request('GET', link1)\n",
        "response_fr = http.request('GET', link2)\n",
        "data_eng = response_eng.data.decode('utf-8')\n",
        "data_fr = response_fr.data.decode('utf-8')\n",
        "english_sentences = data_eng.split('\\n')\n",
        "french_sentences = data_fr.split('\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUll6F0w_OYt",
        "colab_type": "code",
        "outputId": "b14c1c6e-94a5-49eb-9c15-ae06bfbf1fb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "a = len(english_sentences)\n",
        "b = len(french_sentences)\n",
        "print(\" size of english sentences and french sentences\")\n",
        "a,b"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " size of english sentences and french sentences\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(137861, 137861)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aDwGGbY_tEF",
        "colab_type": "text"
      },
      "source": [
        "**Splitting data to training and test datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpDSPxmS_08h",
        "colab_type": "code",
        "outputId": "0db9f4f2-851a-46fb-f2fd-bd0bf92eecb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_eng_data, test_eng_data, train_fr_data, test_fr_data = train_test_split(english_sentences, french_sentences, test_size=0.2)\n",
        "a1 = len(train_eng_data)\n",
        "b1 = len(train_fr_data)\n",
        "print(\"shape of training set : (\"+str(a1)+','+str(b1)+')')\n",
        "\n",
        "a2 = len(test_eng_data)\n",
        "b2 = len(test_fr_data)\n",
        "print(\"shape of validation set : (\"+str(a2)+','+str(b2)+')')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of training set : (110288,110288)\n",
            "shape of test set : (27573,27573)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Uj-zEUJPbih",
        "colab_type": "text"
      },
      "source": [
        "**Exploring the files **\n",
        "\n",
        "Each line in small_vocab_en contains an English sentence with the respective translation in each line of small_vocab_fr. we are going to view the first two lines from each file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmcRMfLvHdjM",
        "colab_type": "code",
        "outputId": "87199cf6-94b6-4c05-bd0e-44057f21da88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "for sample_i in range(5):\n",
        "    print('English sample {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
        "    print('French sample {}:  {}\\n'.format(sample_i + 1, french_sentences[sample_i]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English sample 1:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
            "French sample 1:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
            "\n",
            "English sample 2:  the united states is usually chilly during july , and it is usually freezing in november .\n",
            "French sample 2:  les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
            "\n",
            "English sample 3:  california is usually quiet during march , and it is usually hot in june .\n",
            "French sample 3:  california est généralement calme en mars , et il est généralement chaud en juin .\n",
            "\n",
            "English sample 4:  the united states is sometimes mild during june , and it is cold in september .\n",
            "French sample 4:  les états-unis est parfois légère en juin , et il fait froid en septembre .\n",
            "\n",
            "English sample 5:  your least liked fruit is the grape , but my least liked is the apple .\n",
            "French sample 5:  votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW5htoTTgqBH",
        "colab_type": "text"
      },
      "source": [
        "From looking at the sentences, we can see they have been preprocessed already but they needs more preprocessing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPb3v_Yghg_v",
        "colab_type": "text"
      },
      "source": [
        "Let's explore the vocabulary of the texts and see the most frequent words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRq7rwA7h38K",
        "colab_type": "code",
        "outputId": "21055bbf-68a4-496e-815a-773c78126a8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
        "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
        "\n",
        "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
        "print('{} unique English words.'.format(len(english_words_counter)))\n",
        "print('10 Most common words in the English dataset:')\n",
        "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
        "print()\n",
        "print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
        "print('{} unique French words.'.format(len(french_words_counter)))\n",
        "print('10 Most common words in the French dataset:')\n",
        "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1823250 English words.\n",
            "227 unique English words.\n",
            "10 Most common words in the English dataset:\n",
            "\"is\" \",\" \".\" \"in\" \"it\" \"during\" \"the\" \"but\" \"and\" \"sometimes\"\n",
            "\n",
            "1961295 French words.\n",
            "355 unique French words.\n",
            "10 Most common words in the French dataset:\n",
            "\"est\" \".\" \",\" \"en\" \"il\" \"les\" \"mais\" \"et\" \"la\" \"parfois\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93ecv47Wi-KM",
        "colab_type": "text"
      },
      "source": [
        "**Preprocess**\n",
        "\n",
        "For this project, we won't use text data as input to our model. Instead, we'll convert the text into sequences of integers using the following preprocess methods:\n",
        "\n",
        "Tokenize the words into ids (one-hot encodings) #unordered discrete data\n",
        "Add padding to make all the sequences the same length."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJfOv7kdqSG1",
        "colab_type": "text"
      },
      "source": [
        "**Tokenize (IMPLEMENTATION)**\n",
        "\n",
        "For a neural network to predict on text data, it first has to be turned into data it can understand. Text data like \"dog\" is a sequence of ASCII character encodings. Since a neural network is a series of multiplication and addition operations, the input data needs to be number(s).\n",
        "\n",
        "We can turn each character into a number or each word into a number. These are called character and word ids, respectively. Character ids are used for character level models that generate text predictions for each character. A word level model uses word ids that generate text predictions for each word. Word level models tend to learn better, since they are lower in complexity, so we'll use those.\n",
        "\n",
        "Turn each sentence into a sequence of words ids using Keras's Tokenizer function. Use this function to tokenize english_sentences and french_sentences in the cell below.\n",
        "\n",
        "Running the cell will run tokenize on sample data and show output for debugging."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24xTXXD8qlFC",
        "colab_type": "code",
        "outputId": "b30f489a-b351-483d-d674-2f0f968af076",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "def tokenize(x):\n",
        "\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(x)\n",
        "    return tokenizer.texts_to_sequences(x), tokenizer\n",
        "\n",
        " # Tokenize Example output\n",
        "text_sentences = [\n",
        "    'The quick brown fox jumps over the lazy dog .',\n",
        "    'By Jove , my quick study of lexicography won a prize .',\n",
        "    'This is a short sentence .']\n",
        "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
        "print(text_tokenizer.word_index)\n",
        "print()\n",
        "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
        "    print('Sequence {} in x'.format(sample_i + 1))\n",
        "    print('  Input:  {}'.format(sent))\n",
        "    print('  Output: {}'.format(token_sent))   "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n",
            "\n",
            "Sequence 1 in x\n",
            "  Input:  The quick brown fox jumps over the lazy dog .\n",
            "  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
            "Sequence 2 in x\n",
            "  Input:  By Jove , my quick study of lexicography won a prize .\n",
            "  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
            "Sequence 3 in x\n",
            "  Input:  This is a short sentence .\n",
            "  Output: [18, 19, 3, 20, 21]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X86es_xjuG1t",
        "colab_type": "text"
      },
      "source": [
        "**Padding (IMPLEMENTATION)**\n",
        "\n",
        "When batching the sequence of word ids together, each sequence needs to be the same length. Since sentences are dynamic in length, we can add padding to the end of the sequences to make them the same length.\n",
        "\n",
        "So we make sure all the English sequences have the same length and all the French sequences have the same length by adding padding to the end of each sequence using Keras's pad_sequences function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5_4_KOQuUQd",
        "colab_type": "code",
        "outputId": "6a8a9927-5474-42f6-c1ec-9afb1bc68c50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "\n",
        "def pad(x, length=None):\n",
        "\n",
        "  return pad_sequences(x, maxlen=length, padding='post')\n",
        "\n",
        "# Pad Tokenized output\n",
        "test_pad = pad(text_tokenized)\n",
        "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
        "    print('Sequence {} in x'.format(sample_i + 1))\n",
        "    print('  Input:  {}'.format(np.array(token_sent)))\n",
        "    print('  Output: {}'.format(pad_sent))  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequence 1 in x\n",
            "  Input:  [1 2 4 5 6 7 1 8 9]\n",
            "  Output: [1 2 4 5 6 7 1 8 9 0]\n",
            "Sequence 2 in x\n",
            "  Input:  [10 11 12  2 13 14 15 16  3 17]\n",
            "  Output: [10 11 12  2 13 14 15 16  3 17]\n",
            "Sequence 3 in x\n",
            "  Input:  [18 19  3 20 21]\n",
            "  Output: [18 19  3 20 21  0  0  0  0  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vw6z8J7Mvn4e",
        "colab_type": "text"
      },
      "source": [
        "**Preprocess Pipeline on the whole datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9QrkWHHv2pN",
        "colab_type": "code",
        "outputId": "bacc2c40-ab31-4739-d29d-d1d324e2534f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "def preprocess(x, y):\n",
        "\n",
        "    preprocess_x, x_tk = tokenize(x)\n",
        "    preprocess_y, y_tk = tokenize(y)\n",
        "\n",
        "    preprocess_x = pad(preprocess_x)\n",
        "    preprocess_y = pad(preprocess_y)\n",
        "\n",
        "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
        "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
        "\n",
        "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
        "\n",
        "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer = preprocess(english_sentences, french_sentences)\n",
        "\n",
        "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
        "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
        "english_vocab_size = len(english_tokenizer.word_index)\n",
        "french_vocab_size = len(french_tokenizer.word_index)\n",
        "\n",
        "print('Data Preprocessed')\n",
        "print(\"Max English train sentence length:\", max_english_sequence_length)\n",
        "print(\"Max French train sentence length:\", max_french_sequence_length)\n",
        "print(\"English vocabulary size:\", english_vocab_size)\n",
        "print(\"French vocabulary size:\", french_vocab_size)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Preprocessed\n",
            "Max English train sentence length: 15\n",
            "Max French train sentence length: 21\n",
            "English vocabulary size: 200\n",
            "French vocabulary size: 345\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWEg4Rjh_LFL",
        "colab_type": "text"
      },
      "source": [
        "**Ids Back to Text**\n",
        "\n",
        "The neural network will be translating the input to words ids, which isn't the final form we want. We want the French translation. The function logits_to_text will bridge the gab between the logits from the neural network to the French translation. We'll be using this function to better understand the output of the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9lYFJY8_TNW",
        "colab_type": "code",
        "outputId": "d0b935d8-f684-45a2-a0a1-c5c2a8216e63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def logits_to_text(logits, tokenizer):\n",
        "  \n",
        "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = '<PAD>'\n",
        "\n",
        "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
        "\n",
        "print('`logits_to_text` function loaded.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`logits_to_text` function loaded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6JnFk0O-qmW",
        "colab_type": "text"
      },
      "source": [
        "**Models**\n",
        "\n",
        "In this section, we will experiment with various neural network architectures. we will begin by training four relatively simple architectures.\n",
        "\n",
        "**Model 1 :** is a simple RNN\n",
        "\n",
        "**Model 2 :** is a RNN with Embedding\n",
        "\n",
        "**Model 3 :** is a Bidirectional RNN\n",
        "\n",
        "**Model 4 :** is an optional Encoder-Decoder RNN\n",
        "\n",
        "After experimenting with the four simple architectures, we will construct a deeper architecture that is designed to outperform all four models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEV772AN_DmT",
        "colab_type": "text"
      },
      "source": [
        "**Model 1: Simple RNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLiW5Bva7x2K",
        "colab_type": "code",
        "outputId": "2644ecd5-6017-489f-ff3f-b5acac4ed49e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "source": [
        "\n",
        "def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    \n",
        "    # Hyperparameters\n",
        "    learning_rate = 0.005\n",
        "    \n",
        "    # TODO: Build the layers\n",
        "    model = Sequential()\n",
        "    model.add(GRU(256, input_shape=input_shape[1:], return_sequences=True))\n",
        "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) \n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Reshaping the input to work with a basic RNN\n",
        "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
        "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
        "\n",
        "# Train the neural network\n",
        "simple_rnn_model = simple_model(\n",
        "    tmp_x.shape,\n",
        "    max_french_sequence_length,\n",
        "    english_vocab_size,\n",
        "    french_vocab_size)\n",
        "\n",
        "print(simple_rnn_model.summary())\n",
        "\n",
        "simple_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
        "\n",
        "# Print prediction(s)\n",
        "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))    \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_1 (GRU)                  (None, 21, 256)           198144    \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 21, 1024)          263168    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 21, 1024)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 21, 344)           352600    \n",
            "=================================================================\n",
            "Total params: 813,912\n",
            "Trainable params: 813,912\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 110288 samples, validate on 27573 samples\n",
            "Epoch 1/10\n",
            "110288/110288 [==============================] - 12s 105us/step - loss: 1.9689 - accuracy: 0.5364 - val_loss: nan - val_accuracy: 0.6329\n",
            "Epoch 2/10\n",
            "110288/110288 [==============================] - 9s 85us/step - loss: 1.2322 - accuracy: 0.6422 - val_loss: nan - val_accuracy: 0.6728\n",
            "Epoch 3/10\n",
            "110288/110288 [==============================] - 9s 85us/step - loss: 1.0828 - accuracy: 0.6691 - val_loss: nan - val_accuracy: 0.6787\n",
            "Epoch 4/10\n",
            "110288/110288 [==============================] - 9s 85us/step - loss: 1.0003 - accuracy: 0.6831 - val_loss: nan - val_accuracy: 0.6943\n",
            "Epoch 5/10\n",
            "110288/110288 [==============================] - 9s 85us/step - loss: 0.9419 - accuracy: 0.6934 - val_loss: nan - val_accuracy: 0.7206\n",
            "Epoch 6/10\n",
            "110288/110288 [==============================] - 9s 86us/step - loss: 0.8929 - accuracy: 0.7039 - val_loss: nan - val_accuracy: 0.7338\n",
            "Epoch 7/10\n",
            "110288/110288 [==============================] - 9s 86us/step - loss: 0.8528 - accuracy: 0.7157 - val_loss: nan - val_accuracy: 0.7469\n",
            "Epoch 8/10\n",
            "110288/110288 [==============================] - 10s 88us/step - loss: 0.8513 - accuracy: 0.7108 - val_loss: nan - val_accuracy: 0.7436\n",
            "Epoch 9/10\n",
            "110288/110288 [==============================] - 9s 85us/step - loss: 0.7979 - accuracy: 0.7259 - val_loss: nan - val_accuracy: 0.7319\n",
            "Epoch 10/10\n",
            "110288/110288 [==============================] - 9s 86us/step - loss: 0.7567 - accuracy: 0.7414 - val_loss: nan - val_accuracy: 0.7792\n",
            "new jersey est parfois chaud en mois de il est il est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGPLAUORGSA_",
        "colab_type": "code",
        "outputId": "aa0d78b2-8c24-4c30-af1d-19e2329f145f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Print prediction(s)\n",
        "print(\"Prediction:\")\n",
        "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\n",
        "\n",
        "print(\"\\nCorrect Translation:\")\n",
        "print(french_sentences[:1])\n",
        "\n",
        "print(\"\\nOriginal text:\")\n",
        "print(english_sentences[:1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction:\n",
            "new jersey est parfois chaud en mois de il est il est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            "\n",
            "Correct Translation:\n",
            "[\"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\"]\n",
            "\n",
            "Original text:\n",
            "['new jersey is sometimes quiet during autumn , and it is snowy in april .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQA0Ma6bGhWe",
        "colab_type": "text"
      },
      "source": [
        "**Model 2: Using word Embedding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9ROHrRwGqld",
        "colab_type": "text"
      },
      "source": [
        "We've turned the words into ids, but there's a better representation of a word. This is called word embeddings. An embedding is a vector representation of the word that is close to similar words in n-dimensional space, where the n represents the size of the embedding vectors.\n",
        "\n",
        "In this model, we'll create a RNN model using embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7ci-T3PGmc5",
        "colab_type": "code",
        "outputId": "8df5d384-d91a-44e1-87d4-2dbe2bcf8177",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        }
      },
      "source": [
        "def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    \n",
        "    # Hyperparameters\n",
        "    learning_rate = 0.005\n",
        "    \n",
        "    # TODO: Build the layers\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(english_vocab_size, 256, input_length=input_shape[1], input_shape=input_shape[1:]))\n",
        "    model.add(GRU(256, return_sequences=True))    \n",
        "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) \n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# TODO: Reshape the input\n",
        "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
        "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))\n",
        "\n",
        "# TODO: Train the neural network\n",
        "embed_rnn_model = embed_model(\n",
        "    tmp_x.shape,\n",
        "    preproc_french_sentences.shape[1],\n",
        "    len(english_tokenizer.word_index)+1,\n",
        "    len(french_tokenizer.word_index)+1)\n",
        "\n",
        "embed_rnn_model.summary()\n",
        "\n",
        "embed_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
        "\n",
        "# TODO: Print prediction(s)\n",
        "print(logits_to_text(embed_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 21, 256)           51200     \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (None, 21, 256)           393984    \n",
            "_________________________________________________________________\n",
            "time_distributed_3 (TimeDist (None, 21, 1024)          263168    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 21, 1024)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_4 (TimeDist (None, 21, 345)           353625    \n",
            "=================================================================\n",
            "Total params: 1,061,977\n",
            "Trainable params: 1,061,977\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 110288 samples, validate on 27573 samples\n",
            "Epoch 1/10\n",
            "110288/110288 [==============================] - 11s 98us/step - loss: 1.4097 - accuracy: 0.6740 - val_loss: 0.5197 - val_accuracy: 0.8334\n",
            "Epoch 2/10\n",
            "110288/110288 [==============================] - 10s 91us/step - loss: 0.4318 - accuracy: 0.8584 - val_loss: 0.3232 - val_accuracy: 0.8890\n",
            "Epoch 3/10\n",
            "110288/110288 [==============================] - 10s 92us/step - loss: 0.3081 - accuracy: 0.8967 - val_loss: 0.2535 - val_accuracy: 0.9139\n",
            "Epoch 4/10\n",
            "110288/110288 [==============================] - 10s 91us/step - loss: 0.2541 - accuracy: 0.9141 - val_loss: 0.2245 - val_accuracy: 0.9228\n",
            "Epoch 5/10\n",
            "110288/110288 [==============================] - 10s 92us/step - loss: 0.2260 - accuracy: 0.9226 - val_loss: 0.2089 - val_accuracy: 0.9273\n",
            "Epoch 6/10\n",
            "110288/110288 [==============================] - 10s 92us/step - loss: 0.2110 - accuracy: 0.9271 - val_loss: 0.1980 - val_accuracy: 0.9314\n",
            "Epoch 7/10\n",
            "110288/110288 [==============================] - 10s 92us/step - loss: 0.2004 - accuracy: 0.9300 - val_loss: 0.1955 - val_accuracy: 0.9320\n",
            "Epoch 8/10\n",
            "110288/110288 [==============================] - 10s 94us/step - loss: 0.1937 - accuracy: 0.9320 - val_loss: 0.1886 - val_accuracy: 0.9346\n",
            "Epoch 9/10\n",
            "110288/110288 [==============================] - 10s 93us/step - loss: 0.1853 - accuracy: 0.9345 - val_loss: 0.1862 - val_accuracy: 0.9349\n",
            "Epoch 10/10\n",
            "110288/110288 [==============================] - 10s 93us/step - loss: 0.1822 - accuracy: 0.9351 - val_loss: 0.1816 - val_accuracy: 0.9358\n",
            "new jersey est parfois calme en l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6I-jIS8HkTI",
        "colab_type": "code",
        "outputId": "5bfe51f1-591f-4c55-9027-3491c9da28ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Print prediction(s)\n",
        "print(\"Prediction:\")\n",
        "print(logits_to_text(embed_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\n",
        "\n",
        "print(\"\\nCorrect Translation:\")\n",
        "print(french_sentences[:1])\n",
        "\n",
        "print(\"\\nOriginal text:\")\n",
        "print(english_sentences[:1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction:\n",
            "new jersey est parfois calme en l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            "\n",
            "Correct Translation:\n",
            "[\"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\"]\n",
            "\n",
            "Original text:\n",
            "['new jersey is sometimes quiet during autumn , and it is snowy in april .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UP0KlbElHsnM",
        "colab_type": "text"
      },
      "source": [
        "**Model 3: Bidirectional RNNs**\n",
        "\n",
        "One restriction of a RNN is that it can't see the future input, only the past. This is where bidirectional recurrent neural networks come in. They are able to see the future data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPTro5NRIhCP",
        "colab_type": "code",
        "outputId": "e00d58a8-fb55-4f98-e266-18822db63a8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        }
      },
      "source": [
        "def bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "\n",
        "    # Hyperparameters\n",
        "    learning_rate = 0.003\n",
        "    \n",
        "    # TODO: Build the layers\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(GRU(128, return_sequences=True), input_shape=input_shape[1:]))\n",
        "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) \n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# TODO: Reshape the input\n",
        "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
        "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))\n",
        "\n",
        "# TODO: Train and Print prediction(s)\n",
        "embed_rnn_model = embed_model(\n",
        "    tmp_x.shape,\n",
        "    preproc_french_sentences.shape[1],\n",
        "    len(english_tokenizer.word_index)+1,\n",
        "    len(french_tokenizer.word_index)+1)\n",
        "\n",
        "embed_rnn_model.summary()\n",
        "\n",
        "embed_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
        "\n",
        "print(logits_to_text(embed_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 21, 256)           51200     \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  (None, 21, 256)           393984    \n",
            "_________________________________________________________________\n",
            "time_distributed_5 (TimeDist (None, 21, 1024)          263168    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 21, 1024)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_6 (TimeDist (None, 21, 345)           353625    \n",
            "=================================================================\n",
            "Total params: 1,061,977\n",
            "Trainable params: 1,061,977\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 110288 samples, validate on 27573 samples\n",
            "Epoch 1/10\n",
            "110288/110288 [==============================] - 11s 99us/step - loss: 1.4870 - accuracy: 0.6602 - val_loss: 0.5083 - val_accuracy: 0.8379\n",
            "Epoch 2/10\n",
            "110288/110288 [==============================] - 10s 91us/step - loss: 0.4376 - accuracy: 0.8569 - val_loss: 0.3206 - val_accuracy: 0.8932\n",
            "Epoch 3/10\n",
            "110288/110288 [==============================] - 10s 91us/step - loss: 0.3150 - accuracy: 0.8948 - val_loss: 0.2613 - val_accuracy: 0.9112\n",
            "Epoch 4/10\n",
            "110288/110288 [==============================] - 10s 92us/step - loss: 0.2584 - accuracy: 0.9125 - val_loss: 0.2262 - val_accuracy: 0.9223\n",
            "Epoch 5/10\n",
            "110288/110288 [==============================] - 10s 93us/step - loss: 0.2286 - accuracy: 0.9217 - val_loss: 0.2065 - val_accuracy: 0.9282\n",
            "Epoch 6/10\n",
            "110288/110288 [==============================] - 10s 93us/step - loss: 0.2138 - accuracy: 0.9262 - val_loss: 0.2006 - val_accuracy: 0.9301\n",
            "Epoch 7/10\n",
            "110288/110288 [==============================] - 10s 93us/step - loss: 0.2024 - accuracy: 0.9297 - val_loss: 0.1925 - val_accuracy: 0.9325\n",
            "Epoch 8/10\n",
            "110288/110288 [==============================] - 10s 94us/step - loss: 0.1943 - accuracy: 0.9320 - val_loss: 0.1884 - val_accuracy: 0.9336\n",
            "Epoch 9/10\n",
            "110288/110288 [==============================] - 10s 95us/step - loss: 0.1888 - accuracy: 0.9335 - val_loss: 0.1870 - val_accuracy: 0.9349\n",
            "Epoch 10/10\n",
            "110288/110288 [==============================] - 10s 94us/step - loss: 0.1834 - accuracy: 0.9350 - val_loss: 0.1842 - val_accuracy: 0.9354\n",
            "new jersey est parfois calme en l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_lTukLHJQu6",
        "colab_type": "code",
        "outputId": "91bb2b6b-70eb-46bf-b3ff-538431785791",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "print(\"Prediction:\")\n",
        "print(logits_to_text(embed_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\n",
        "\n",
        "print(\"\\nCorrect Translation:\")\n",
        "print(french_sentences[:1])\n",
        "\n",
        "print(\"\\nOriginal text:\")\n",
        "print(english_sentences[:1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction:\n",
            "new jersey est parfois calme en l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            "\n",
            "Correct Translation:\n",
            "[\"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\"]\n",
            "\n",
            "Original text:\n",
            "['new jersey is sometimes quiet during autumn , and it is snowy in april .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJmRymS6JVmU",
        "colab_type": "text"
      },
      "source": [
        "**Model 4: Encoder-Decoder**\n",
        "\n",
        "This model is made up of an encoder and decoder. The encoder creates a matrix representation of the sentence. The decoder takes this matrix as input and predicts the translation as output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDS9bCrbJixa",
        "colab_type": "code",
        "outputId": "a9f3eceb-0d04-4c77-da00-784962629822",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "def encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "  \n",
        "    # Hyperparameters\n",
        "    learning_rate = 0.001\n",
        "    \n",
        "    # Build the layers    \n",
        "    model = Sequential()\n",
        "    # Encoder\n",
        "    model.add(GRU(256, input_shape=input_shape[1:], go_backwards=True))\n",
        "    model.add(RepeatVector(output_sequence_length))\n",
        "    # Decoder\n",
        "    model.add(GRU(256, return_sequences=True))\n",
        "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Reshape the input\n",
        "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
        "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
        "\n",
        "# Train and Print prediction(s)\n",
        "encdec_rnn_model = encdec_model(\n",
        "    tmp_x.shape,\n",
        "    preproc_french_sentences.shape[1],\n",
        "    len(english_tokenizer.word_index)+1,\n",
        "    len(french_tokenizer.word_index)+1)\n",
        "\n",
        "encdec_rnn_model.summary()\n",
        "\n",
        "encdec_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_4 (GRU)                  (None, 256)               198144    \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 21, 256)           0         \n",
            "_________________________________________________________________\n",
            "gru_5 (GRU)                  (None, 21, 256)           393984    \n",
            "_________________________________________________________________\n",
            "time_distributed_7 (TimeDist (None, 21, 1024)          263168    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 21, 1024)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_8 (TimeDist (None, 21, 345)           353625    \n",
            "=================================================================\n",
            "Total params: 1,208,921\n",
            "Trainable params: 1,208,921\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 110288 samples, validate on 27573 samples\n",
            "Epoch 1/10\n",
            "110288/110288 [==============================] - 19s 169us/step - loss: 2.4886 - accuracy: 0.4727 - val_loss: 1.7926 - val_accuracy: 0.5702\n",
            "Epoch 2/10\n",
            "110288/110288 [==============================] - 18s 162us/step - loss: 1.6552 - accuracy: 0.5746 - val_loss: 1.4467 - val_accuracy: 0.6075\n",
            "Epoch 3/10\n",
            "110288/110288 [==============================] - 18s 160us/step - loss: 1.4383 - accuracy: 0.6088 - val_loss: 1.3478 - val_accuracy: 0.6330\n",
            "Epoch 4/10\n",
            "110288/110288 [==============================] - 18s 162us/step - loss: 1.3532 - accuracy: 0.6278 - val_loss: 1.2721 - val_accuracy: 0.6486\n",
            "Epoch 5/10\n",
            "110288/110288 [==============================] - 18s 160us/step - loss: 1.2945 - accuracy: 0.6405 - val_loss: 1.2490 - val_accuracy: 0.6504\n",
            "Epoch 6/10\n",
            "110288/110288 [==============================] - 18s 163us/step - loss: 1.2458 - accuracy: 0.6497 - val_loss: 1.1830 - val_accuracy: 0.6640\n",
            "Epoch 7/10\n",
            "110288/110288 [==============================] - 18s 162us/step - loss: 1.2090 - accuracy: 0.6564 - val_loss: 1.1609 - val_accuracy: 0.6681\n",
            "Epoch 8/10\n",
            "110288/110288 [==============================] - 18s 167us/step - loss: 1.1717 - accuracy: 0.6649 - val_loss: 1.1248 - val_accuracy: 0.6784\n",
            "Epoch 9/10\n",
            "110288/110288 [==============================] - 18s 161us/step - loss: 1.1408 - accuracy: 0.6713 - val_loss: 1.0774 - val_accuracy: 0.6878\n",
            "Epoch 10/10\n",
            "110288/110288 [==============================] - 18s 161us/step - loss: 1.1173 - accuracy: 0.6732 - val_loss: 1.0359 - val_accuracy: 0.6937\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f3b5b958898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1c9N2yIKj6S",
        "colab_type": "code",
        "outputId": "b02800fe-df71-49a6-91f9-94b0baacdb9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Print prediction(s)\n",
        "print(\"Prediction:\")\n",
        "print(logits_to_text(encdec_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\n",
        "\n",
        "print(\"\\nCorrect Translation:\")\n",
        "print(french_sentences[:1])\n",
        "\n",
        "print(\"\\nOriginal text:\")\n",
        "print(english_sentences[:1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction:\n",
            "new jersey est parfois agréable en l' et il est est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            "\n",
            "Correct Translation:\n",
            "[\"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\"]\n",
            "\n",
            "Original text:\n",
            "['new jersey is sometimes quiet during autumn , and it is snowy in april .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6Vg49eRKlXp",
        "colab_type": "text"
      },
      "source": [
        "**Model 5: RNN using all above**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcxoOb67KyQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "\n",
        "    # Hyperparameters\n",
        "    learning_rate = 0.003\n",
        "    \n",
        "    # Build the layers    \n",
        "    model = Sequential()\n",
        "    # Embedding\n",
        "    model.add(Embedding(english_vocab_size, 128, input_length=input_shape[1],\n",
        "                         input_shape=input_shape[1:]))\n",
        "    # Encoder\n",
        "    model.add(Bidirectional(GRU(128)))\n",
        "    model.add(RepeatVector(output_sequence_length))\n",
        "    # Decoder\n",
        "    model.add(Bidirectional(GRU(128, return_sequences=True)))\n",
        "    model.add(TimeDistributed(Dense(512, activation='relu')))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgfFX7n_LOTt",
        "colab_type": "text"
      },
      "source": [
        "**Final Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wt6Cd0mrLPD5",
        "colab_type": "code",
        "outputId": "cc3e2d0b-1e4b-480b-ac12-1c40f8e49e8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def final_predictions(x, y, x_tk, y_tk):\n",
        "\n",
        "    # TODO: Train neural network using model_final\n",
        "    model = model_final(x.shape,y.shape[1],\n",
        "                        len(x_tk.word_index)+1,\n",
        "                        len(y_tk.word_index)+1)\n",
        "    model.summary()\n",
        "    model.fit(x, y, batch_size=1024, epochs=25, validation_split=0.2)\n",
        "\n",
        "    \n",
        "    ## Test sample\n",
        "    y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
        "    y_id_to_word[0] = '<PAD>'\n",
        "\n",
        "    sentence = 'he saw a old yellow truck'\n",
        "    sentence = [x_tk.word_index[word] for word in sentence.split()]\n",
        "    sentence = pad_sequences([sentence], maxlen=x.shape[-1], padding='post')\n",
        "    sentences = np.array([sentence[0], x[0]])\n",
        "    predictions = model.predict(sentences, len(sentences))\n",
        "\n",
        "    print('Sample 1:')\n",
        "    print('he saw a old yellow truck')\n",
        "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))\n",
        "    #print('Il a vu un vieux camion jaune')\n",
        "    #print('Sample 2:')\n",
        "    #print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[1]]))\n",
        "    #print(' '.join([y_id_to_word[np.max(x)] for x in y[0]]))\n",
        "\n",
        "final_predictions(preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer)    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 15, 128)           25600     \n",
            "_________________________________________________________________\n",
            "bidirectional_9 (Bidirection (None, 256)               197376    \n",
            "_________________________________________________________________\n",
            "repeat_vector_5 (RepeatVecto (None, 21, 256)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_10 (Bidirectio (None, 21, 256)           295680    \n",
            "_________________________________________________________________\n",
            "time_distributed_9 (TimeDist (None, 21, 512)           131584    \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 21, 512)           0         \n",
            "_________________________________________________________________\n",
            "time_distributed_10 (TimeDis (None, 21, 345)           176985    \n",
            "=================================================================\n",
            "Total params: 827,225\n",
            "Trainable params: 827,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 110288 samples, validate on 27573 samples\n",
            "Epoch 1/25\n",
            "110288/110288 [==============================] - 38s 342us/step - loss: 2.4466 - accuracy: 0.4811 - val_loss: 1.5981 - val_accuracy: 0.5917\n",
            "Epoch 2/25\n",
            "110288/110288 [==============================] - 36s 329us/step - loss: 1.4375 - accuracy: 0.6187 - val_loss: 1.1545 - val_accuracy: 0.6810\n",
            "Epoch 3/25\n",
            "110288/110288 [==============================] - 36s 328us/step - loss: 1.1408 - accuracy: 0.6782 - val_loss: 0.9739 - val_accuracy: 0.7169\n",
            "Epoch 4/25\n",
            "110288/110288 [==============================] - 36s 327us/step - loss: 1.0277 - accuracy: 0.7017 - val_loss: 0.8438 - val_accuracy: 0.7450\n",
            "Epoch 5/25\n",
            "110288/110288 [==============================] - 36s 322us/step - loss: 0.8660 - accuracy: 0.7380 - val_loss: 0.7323 - val_accuracy: 0.7747\n",
            "Epoch 6/25\n",
            "110288/110288 [==============================] - 36s 322us/step - loss: 0.7909 - accuracy: 0.7556 - val_loss: 0.6315 - val_accuracy: 0.8024\n",
            "Epoch 7/25\n",
            "110288/110288 [==============================] - 35s 322us/step - loss: 0.6595 - accuracy: 0.7920 - val_loss: 0.5499 - val_accuracy: 0.8276\n",
            "Epoch 8/25\n",
            "110288/110288 [==============================] - 36s 324us/step - loss: 0.5853 - accuracy: 0.8142 - val_loss: 0.4590 - val_accuracy: 0.8566\n",
            "Epoch 9/25\n",
            "110288/110288 [==============================] - 36s 323us/step - loss: 0.4892 - accuracy: 0.8462 - val_loss: 0.3807 - val_accuracy: 0.8859\n",
            "Epoch 10/25\n",
            "110288/110288 [==============================] - 35s 321us/step - loss: 0.4855 - accuracy: 0.8494 - val_loss: 0.4029 - val_accuracy: 0.8783\n",
            "Epoch 11/25\n",
            "110288/110288 [==============================] - 35s 322us/step - loss: 0.3823 - accuracy: 0.8827 - val_loss: 0.2757 - val_accuracy: 0.9188\n",
            "Epoch 12/25\n",
            "110288/110288 [==============================] - 36s 322us/step - loss: 0.3139 - accuracy: 0.9046 - val_loss: 0.2258 - val_accuracy: 0.9342\n",
            "Epoch 13/25\n",
            "110288/110288 [==============================] - 36s 323us/step - loss: 0.2718 - accuracy: 0.9182 - val_loss: 0.1851 - val_accuracy: 0.9498\n",
            "Epoch 14/25\n",
            "110288/110288 [==============================] - 35s 321us/step - loss: 0.2280 - accuracy: 0.9329 - val_loss: 0.1650 - val_accuracy: 0.9563\n",
            "Epoch 15/25\n",
            "110288/110288 [==============================] - 35s 320us/step - loss: 0.2146 - accuracy: 0.9363 - val_loss: 0.1505 - val_accuracy: 0.9578\n",
            "Epoch 16/25\n",
            "110288/110288 [==============================] - 35s 322us/step - loss: 0.1794 - accuracy: 0.9471 - val_loss: 0.1291 - val_accuracy: 0.9630\n",
            "Epoch 17/25\n",
            "110288/110288 [==============================] - 36s 323us/step - loss: 0.1741 - accuracy: 0.9479 - val_loss: 0.1298 - val_accuracy: 0.9630\n",
            "Epoch 18/25\n",
            "110288/110288 [==============================] - 36s 322us/step - loss: 0.3184 - accuracy: 0.9040 - val_loss: 0.1314 - val_accuracy: 0.9639\n",
            "Epoch 19/25\n",
            "110288/110288 [==============================] - 36s 322us/step - loss: 0.1644 - accuracy: 0.9522 - val_loss: 0.1213 - val_accuracy: 0.9661\n",
            "Epoch 20/25\n",
            "110288/110288 [==============================] - 36s 323us/step - loss: 0.1445 - accuracy: 0.9576 - val_loss: 0.1064 - val_accuracy: 0.9703\n",
            "Epoch 21/25\n",
            "110288/110288 [==============================] - 36s 322us/step - loss: 0.1289 - accuracy: 0.9621 - val_loss: 0.1063 - val_accuracy: 0.9703\n",
            "Epoch 22/25\n",
            "110288/110288 [==============================] - 35s 319us/step - loss: 0.1269 - accuracy: 0.9628 - val_loss: 0.0961 - val_accuracy: 0.9728\n",
            "Epoch 23/25\n",
            "110288/110288 [==============================] - 36s 322us/step - loss: 0.1128 - accuracy: 0.9667 - val_loss: 0.0948 - val_accuracy: 0.9743\n",
            "Epoch 24/25\n",
            "110288/110288 [==============================] - 35s 320us/step - loss: 0.1103 - accuracy: 0.9675 - val_loss: 0.1001 - val_accuracy: 0.9727\n",
            "Epoch 25/25\n",
            "110288/110288 [==============================] - 36s 323us/step - loss: 0.1068 - accuracy: 0.9684 - val_loss: 0.0853 - val_accuracy: 0.9763\n",
            "Sample 1:\n",
            "he saw a old yellow truck\n",
            "il a vu un vieux camion jaune <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}